from __future__ import annotations

from typing import Iterable

import onnx_ir as ir

def make_external_tensor(name: str, dtype: ir.DataType, shape: Iterable[int | str]) -> ir.ExternalTensor:
    return ir.ExternalTensor(
        location=name,
        offset=None,
        length=None,
        dtype=dtype,
        shape=ir.Shape(list(shape)),
        name=name
    )from __future__ import annotations

from typing import Any, Callable, Sequence
from contextlib import contextmanager
from threading import local

import onnx_ir as ir
import onnx_ir.passes.common as common_passes

class IRModelBuilder(ir.tape.Tape):
    def __init__(self) -> None:
        super().__init__()
        self.op_builder = OpBuilder(self)
        self._module_stack : list[BuilderModule] = []

    def push_module(self, module: str) -> None:
        self._module_stack.append(module)

    def pop_module(self) -> None:
        self._module_stack.pop()

    def context_name(self) -> str:
        return ".".join([m.name for m in self._module_stack if m.name])

    def connect_output(self, output_parameter: ir.Value, computed_value: ir.Value) -> None:
        """Connect a computed value to an output parameter of the graph.
        
        Args:
            output_parameter: This specifies the name, type, and shape of the output.
            computed_value: The computed value to connect to the output parameter.
        """
        if computed_value.producer() is None:
            # Graph input. Can't use _is_graph_input method yet.
            # Identity-elimination is not possible in this case
            self.op("Identity", inputs=[computed_value], output=output_parameter)
        else:
            # Avoid unnecessary Identity nodes
            # TODO: Check type and shape compatibility
            computed_value.name = output_parameter.name
            computed_value.type = output_parameter.type
            computed_value.shape = output_parameter.shape

    def initializer(self, tensor: ir.TensorProtocol, name: str | None = None) -> ir.Value:
        if name is None:
            name = tensor.name
        prefix = self.context_name()
        if prefix:
            name = f"{prefix}.{name}"
            # TODO: set tensor name as well
        return super().initializer(tensor, name=name)

# Global thread-local storage for builder stack
_thread_local = local()


def get_current_builder() -> IRModelBuilder:
    """Get the current IRModelBuilder from the context stack."""
    stack = getattr(_thread_local, 'builder_stack', None)
    if stack:
        return stack[-1]
    else:
        raise RuntimeError("No active IRModelBuilder found in context.")

def get_current_op_builder() -> OpBuilder:
    """Get the current OpBuilder from the context stack."""
    builder = get_current_builder()
    return builder.op_builder

@contextmanager
def builder_context(builder: IRModelBuilder):
    """Context manager to set the current IRModelBuilder.
    
    Args:
        builder: The IRModelBuilder to set as current
        
    Usage:
        with builder_context(my_builder):
            # my_builder is now the current builder
            current = get_current_builder()
    """
    # Initialize the stack if it doesn't exist
    if not hasattr(_thread_local, 'builder_stack'):
        _thread_local.builder_stack = []
    
    # Push the new builder onto the stack
    _thread_local.builder_stack.append(builder)
    try:
        yield builder
    finally:
        # Pop the builder from the stack
        _thread_local.builder_stack.pop()

class OpBuilder:
    def __init__(self, builder: IRModelBuilder) -> None:
        self._builder = builder
    
    def __getattr__(self, op_type: str) -> Any:
        return lambda *args, **kwargs: self._make_node(op_type, args, kwargs)

    def _adapt_input(self, value: ir.Value | ir.TensorProtocol) -> ir.Value:
        if not isinstance(value, ir.Value):
            # TODO: We could using caching to avoid duplicate initializers. However, it seems unlikely
            # to be useful in practice, as shared use of a stateful module is rare.
            return self._builder.initializer(value)
        return value

    def _adapt_outputs(self, outputs: int | Sequence[str | ir.Value]) -> Sequence[ir.Value]:
        prefix = self._builder.context_name()
        if isinstance(outputs, int):
            count = len(self._builder.nodes)
            name = f"{prefix}.val_{count}" if prefix else "val_{count}"
            if outputs == 1:
                return [ir.Value(name=name)]
            else:
                return [ir.Value(name=f"{name}.{i}") for i in range(outputs)]
        adapted_outputs = []
        for output in outputs:
            if isinstance(output, ir.Value):
                adapted_outputs.append(output)
            else:
                adapted_outputs.append(ir.Value(name=output))
        return adapted_outputs
    
    def _make_node(self, op_type: str, inputs: Sequence[ir.Value | ir.TensorProtocol], kwargs: dict[str, Any]):
        domain = kwargs.pop("_domain", "")
        version = kwargs.pop("_version", None)
        outputs = kwargs.pop("_outputs", 1)

        output_values = self._adapt_outputs(outputs)

        inputs = [self._adapt_input(i) for i in inputs]

        if len(output_values) == 1:
            value = self._builder.op(
                op_type, inputs=inputs, attributes=kwargs, domain=domain, version=version, output=output_values[0]
            )
            if isinstance(outputs, Sequence):
                value.name = outputs[0]
            return value
        values = self._builder.op_multi_out(
            op_type,
            inputs=inputs,
            attributes=kwargs,
            domain=domain,
            version=version,
            outputs=output_values,
        )
        return values

class BuilderModule:
    def __init__(self, name: str | None = None):
        """Initialize BuilderModule with optional name.
        
        Args:
            name: Optional name for the module. If None, uses the class name.
        """
        # self.name = name if name is not None else self.__class__.__name__
        self.name = name

    def __call__(self, *args, **kwargs):
        """Delegate calls to the forward method."""
        self.builder = get_current_builder()
        self.op = get_current_op_builder()
        self.builder.push_module(self)
        result = self.forward(*args, **kwargs)
        self.builder.pop_module()
        return result
    
    def forward(self, *args, **kwargs):
        """Forward method to be implemented by subclasses."""
        raise NotImplementedError("Subclasses must implement the forward method")
    

GraphBuilderFunction = Callable[[Sequence[ir.Value]], Sequence[ir.Value]]

def export(model: GraphBuilderFunction, model_inputs: Sequence[ir.Value], model_outputs: Sequence[ir.Value], model_id: str) -> ir.Model:
    # opset_version=23,
    builder = IRModelBuilder()
    with builder_context(builder):
        outputs = model(model_inputs)
        assert len(outputs) == len(model_outputs), "Output length mismatch"
        for output_parameter, computed_value in zip(model_outputs, outputs):
            builder.connect_output(output_parameter=output_parameter, computed_value=computed_value)
        graph = ir.Graph(
            name=f"{model_id}",
            inputs=model_inputs,
            outputs=model_outputs,
            nodes=builder.nodes,
            initializers=builder.initializers,
            opset_imports={"": 23},
        )
        onnx_model = ir.Model(
            graph=graph,
            ir_version=11,
            producer_name="onnx_models",
            producer_version="0.1.0",
        )

    # passes = ir.passes.PassManager(
    #     [
    #         # onnx_passes.AssignNamesPass(),
    #         # onnx_passes.FoldTransposePass(),
    #         common_passes.RemoveUnusedNodesPass(),
    #         common_passes.RemoveUnusedFunctionsPass(),
    #         common_passes.RemoveUnusedOpsetsPass(),
    #         common_passes.DeduplicateInitializersPass(),
    #         common_passes.CommonSubexpressionEliminationPass(),
    #         # onnx_passes.RemoveBarrierPass()
    #     ]
    # )

    # passes(onnx_model)
    return onnx_model
from __future__ import annotations

import dataclasses

import torch


# https://github.com/huggingface/transformers/blob/3e975acc8bf6d029ec0a54b1c5d0691489dfb051/src/transformers/models/auto/configuration_auto.py#L36
SUPPORTED_ARCHITECTURES = {
    # "ernie4_5",
    # "gemma",
    # "gemma2",
    # "gemma3",
    # "gptoss",
    # "granite",
    "llama",
    "mistral",
    # "nemotron",
    # "olmo",
    # "phi",
    # "phi3",
    # "phi3small",
    # "phi3v",
    # "phi4mm",
    # "phimoe",
    # "qwen2",
    # "qwen3",
    # "smollm3",
}


DEFAULT_INT = -42


@dataclasses.dataclass
class ArchitectureConfig:
    # Config from transformers
    vocab_size: int = DEFAULT_INT
    max_position_embeddings: int = DEFAULT_INT
    hidden_size: int = DEFAULT_INT
    intermediate_size: int = DEFAULT_INT
    num_hidden_layers: int = DEFAULT_INT
    num_attention_heads: int = DEFAULT_INT
    num_key_value_heads: int = DEFAULT_INT
    hidden_act: str | None = None

    rms_norm_eps: float = 1e-6

    # Rotary embedding config
    rope_type: str = "default"
    rope_theta: float = 10_000.0
    rope_scaling: dict | None = None
    partial_rotary_factor: float = 1.0  # 1.0 means no partial RoPE

    attention_bias: bool = False
    mlp_bias: bool = False

    head_dim: int = DEFAULT_INT

    pad_token_id: int = DEFAULT_INT

    @classmethod
    def from_transformers(cls, config) -> ArchitectureConfig:
        if config.model_type not in SUPPORTED_ARCHITECTURES:
            raise ValueError(
                f"Model type '{config.model_type}' not supported. Supported architectures: {SUPPORTED_ARCHITECTURES}"
            )

        options = dict(
            head_dim=config.hidden_size // config.num_attention_heads,
            num_attention_heads=config.num_attention_heads,
            num_key_value_heads=(
                getattr(config, "num_key_value_heads", config.num_attention_heads)
            ),
            num_hidden_layers=config.num_hidden_layers,
            vocab_size=config.vocab_size,
            hidden_size=config.hidden_size,
            intermediate_size=(
                getattr(config, "intermediate_size", 4 * config.hidden_size)
            ),
            hidden_act=(getattr(config, "hidden_act", None)),
            pad_token_id=(getattr(config, "pad_token_id", 0)),  # FIXME
            rms_norm_eps=(getattr(config, "rms_norm_eps", 1e-6)),
            attention_bias=(getattr(config, "add_bias_kv", False)),
            mlp_bias=(getattr(config, "use_mlp_bias", False)),
            # how much older transformers versions are we supporting?
            rope_type=(config.rope_scaling.get("rope_type") if hasattr(config, "rope_scaling") and isinstance(config.rope_scaling, dict) else "default"),
            rope_theta=(getattr(config, "rope_theta", 10_000.0)),
            rope_scaling=(getattr(config, "rope_scaling", None)),
            partial_rotary_factor=(getattr(config, "partial_rotary_factor", 1.0)),
            max_position_embeddings=config.max_position_embeddings,
        )

        return cls(**options)


@dataclasses.dataclass
class ExportConfig:
    nothing_yet: bool = True
from __future__ import annotations

__all__ = ["convert_hf_model"]

import logging

import onnx_ir as ir

from onnx_models import _configs
from onnx_models.components._model import CausalLMModel
from onnx_models._builder import export

logger = logging.getLogger(__name__)

def _create_inputs_outputs(
    config: _configs.ArchitectureConfig, export_config: _configs.ExportConfig
):
    """Create ONNX IR Values for inputs and outputs based on the model configuration."""
    num_hidden_layers = config.num_hidden_layers
    num_key_value_heads = config.num_key_value_heads
    head_dim = config.head_dim
    
    # Create input Values with appropriate shapes and types
    input_values = {}
    
    # Input IDs: (batch_size, sequence_length) - int64
    input_ids = ir.Value(
        name="input_ids",
        shape=ir.Shape(["batch", "sequence_len"]),
        type=ir.TensorType(ir.DataType.INT64)
    )
    
    # Attention mask: (batch_size, past_sequence_len + sequence_len) - int64  
    attention_mask = ir.Value(
        name="attention_mask", 
        shape=ir.Shape(["batch", "past_sequence_len+sequence_len"]),
        type=ir.TensorType(ir.DataType.INT64)
    )
    
    # Position IDs: (batch_size, sequence_length) - int64
    position_ids = ir.Value(
        name="position_ids",
        shape=ir.Shape(["batch", "sequence_len"]), 
        type=ir.TensorType(ir.DataType.INT64)
    )
    
    input_values = [input_ids, attention_mask, position_ids]

    # Past key values: list of (key, value) tuples for each layer
    # Each has shape (batch_size, num_key_value_heads, past_sequence_len, head_dim) - float32
    # Create flat list directly
    for i in range(num_hidden_layers):
        input_values.extend([
            ir.Value(
                name=f"past_key_values.{i}.key",
                shape=ir.Shape(["batch", num_key_value_heads, "past_sequence_len", head_dim]),
                type=ir.TensorType(ir.DataType.FLOAT)
            ),
            ir.Value(
                name=f"past_key_values.{i}.value", 
                shape=ir.Shape(["batch", num_key_value_heads, "past_sequence_len", head_dim]),
                type=ir.TensorType(ir.DataType.FLOAT)
            )
        ])
    
    # Create output Values
    
    # Logits: (batch_size, sequence_length, vocab_size) - float32
    logits = ir.Value(
        name="logits",
        shape=ir.Shape(["batch", "sequence_len", config.vocab_size]),
        type=ir.TensorType(ir.DataType.FLOAT)
    )
    
    # Present key values: list of (key, value) tuples for each layer  
    # Each has shape (batch_size, num_key_value_heads, sequence_len, head_dim) - float32
    output_values = [logits]
    for i in range(num_hidden_layers):
        output_values.extend([
            ir.Value(
                name=f"present.{i}.key",
                shape=ir.Shape(["batch", num_key_value_heads, "sequence_len", head_dim]),
                type=ir.TensorType(ir.DataType.FLOAT)
            ),
            ir.Value(
                name=f"present.{i}.value",
                shape=ir.Shape(["batch", num_key_value_heads, "sequence_len", head_dim]), 
                type=ir.TensorType(ir.DataType.FLOAT)
            )
        ])
    
    return input_values, output_values



def convert_hf_model(
    model_id: str = "meta-llama/Llama-3.2-1B-Instruct",
    load_weights: bool = True,
    clear_metadata: bool = False,
) -> ir.Model:
    """Convert a HuggingFace model to ONNX.

    Args:
        model_id: The model ID on HuggingFace Hub.
        load_weights: Whether to load the pretrained weights from the HuggingFace model.
        clear_metadata: Whether to clear debugging metadata from the ONNX model.
    """
    import transformers

    # Need to use transformers to load config because transformers has additional
    # logic to standardize the config field names.
    config = transformers.AutoConfig.from_pretrained(model_id)
    architecture_config = _configs.ArchitectureConfig.from_transformers(config)
    
    model = CausalLMModel(architecture_config)
    model_inputs, model_outputs = _create_inputs_outputs(
        architecture_config, None
    )

    def adapted_model(inputs):
        structured_input = model.unflatten_inputs(inputs)
        structured_output = model(*structured_input)
        flattened_output = model.flatten_outputs(structured_output)
        return flattened_output
    
    onnx_model = export(adapted_model, model_inputs, model_outputs, model_id)

    return onnx_model
from __future__ import annotations

__all__ = ["ArchitectureConfig", "ExportConfig", "components", "BuilderModule"]

from ._builder import BuilderModule
from . import components
from ._configs import ArchitectureConfig, ExportConfig

